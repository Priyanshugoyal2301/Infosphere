"""
Integration Test Suite for Infosphere ML Pipeline

This script tests the complete ML pipeline including:
- Database connectivity
- Model training
- News classification  
- Backend integration

Run this script to validate the entire system is working correctly.

Author: Infosphere Team
Date: October 2025
"""

import sys
import os
import time
from datetime import datetime

# Add backend path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_path = os.path.join(current_dir, 'backend')
if backend_path not in sys.path:
    sys.path.append(backend_path)

def test_database_connectivity():
    """Test database connector functionality."""
    print("ğŸ” Testing Database Connectivity...")
    
    try:
        from backend.db_connector import NewsDataConnector, fetch_user_data
        
        # Test connector initialization
        connector = NewsDataConnector()
        print("  âœ… Database connector initialized successfully")
        
        # Test data fetching
        df = fetch_user_data()
        if len(df) > 0:
            print(f"  âœ… Fetched {len(df)} records from database")
            print(f"  ğŸ“Š Categories: {df['label'].unique().tolist()}")
        else:
            print("  âŒ No data found in database")
            return False
            
        # Test label distribution
        distribution = connector.get_label_distribution()
        print(f"  ğŸ“ˆ Label distribution: {distribution}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Database test failed: {e}")
        return False


def test_model_training():
    """Test ML model training pipeline."""
    print("\nğŸš€ Testing Model Training Pipeline...")
    
    try:
        # Import training functions
        import subprocess
        import pickle
        
        # Check if model files already exist
        model_dir = os.path.join(current_dir, 'ml_model')
        model_files = [
            'news_classifier_model.pkl',
            'tfidf_vectorizer.pkl', 
            'model_metadata.pkl'
        ]
        
        files_exist = all(os.path.exists(os.path.join(model_dir, f)) for f in model_files)
        
        if not files_exist:
            print("  ğŸ”„ Model files not found, training new model...")
            
            # Run training script
            result = subprocess.run([
                sys.executable, 
                os.path.join(model_dir, 'train_from_website.py')
            ], capture_output=True, text=True)
            
            if result.returncode != 0:
                print(f"  âŒ Training failed: {result.stderr}")
                return False
            else:
                print("  âœ… Model training completed successfully")
        else:
            print("  âœ… Model files found")
            
        # Test loading model metadata
        metadata_path = os.path.join(model_dir, 'model_metadata.pkl')
        with open(metadata_path, 'rb') as f:
            metadata = pickle.load(f)
            
        print(f"  ğŸ“Š Model accuracy: {metadata.get('accuracy', 'Unknown')}")
        print(f"  ğŸ—“ï¸ Training date: {metadata.get('training_date', 'Unknown')}")
        print(f"  ğŸ·ï¸ Categories: {metadata.get('classes', [])}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Model training test failed: {e}")
        return False


def test_news_classification():
    """Test news classification functionality."""
    print("\nğŸ¯ Testing News Classification...")
    
    try:
        from backend.analyze_input import analyze_news_input, get_model_status
        
        # Check model status
        status = get_model_status()
        if not status['model_loaded']:
            print("  âŒ Model not loaded")
            return False
            
        print(f"  âœ… Model loaded successfully")
        print(f"  ğŸ“Š Model type: {status.get('model_type', 'Unknown')}")
        
        # Test sample classifications
        test_cases = [
            ("Fire broke out in commercial building", "Accident"),
            ("Free health checkup camp organized", "Event"), 
            ("Heavy rainfall expected tomorrow", "Weather"),
            ("Theft reported near metro station", "Crime"),
            ("Blood donation drive this weekend", "Event"),
            ("Car accident on highway", "Accident"),
            ("Thunderstorm warning issued", "Weather"),
            ("Burglary attempt at shop", "Crime")
        ]
        
        print("  ğŸ§ª Testing sample classifications:")
        correct_predictions = 0
        
        for text, expected_category in test_cases:
            result = analyze_news_input(text)
            
            if 'error' in result:
                print(f"    âŒ '{text[:30]}...' â†’ Error: {result['error']}")
                continue
                
            predicted = result['category']
            confidence = result['confidence']
            is_correct = predicted == expected_category
            
            if is_correct:
                correct_predictions += 1
                
            status_icon = "âœ…" if is_correct else "âš ï¸"
            print(f"    {status_icon} '{text[:30]}...' â†’ {predicted} (conf: {confidence:.3f})")
            
        accuracy = correct_predictions / len(test_cases) if test_cases else 0
        print(f"  ğŸ“ˆ Test accuracy: {accuracy:.2%} ({correct_predictions}/{len(test_cases)})")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Classification test failed: {e}")
        return False


def test_backend_integration():
    """Test backend integration components."""
    print("\nğŸ”— Testing Backend Integration...")
    
    try:
        from backend.analyze_input import (
            get_classifier, 
            batch_analyze_news,
            get_model_status
        )
        
        # Test singleton classifier
        classifier1 = get_classifier()
        classifier2 = get_classifier()
        
        if classifier1 is classifier2:
            print("  âœ… Singleton classifier pattern working")
        else:
            print("  âš ï¸ Multiple classifier instances created")
            
        # Test batch processing
        batch_texts = [
            "Fire at factory", 
            "Concert tonight",
            "Rain warning",
            "Bank robbery"
        ]
        
        batch_results = batch_analyze_news(batch_texts)
        
        if batch_results['status'] == 'success':
            successful = batch_results['successful_predictions']
            total = batch_results['total_processed']
            print(f"  âœ… Batch processing: {successful}/{total} successful")
        else:
            print(f"  âŒ Batch processing failed: {batch_results.get('error', 'Unknown')}")
            
        # Test model info
        status = get_model_status()
        print(f"  ğŸ“‹ Model info available: {len(status)} fields")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Backend integration test failed: {e}")
        return False


def test_error_handling():
    """Test error handling and edge cases."""
    print("\nğŸ›¡ï¸ Testing Error Handling...")
    
    try:
        from backend.analyze_input import analyze_news_input
        
        # Test empty input
        result = analyze_news_input("")
        if 'error' in result:
            print("  âœ… Empty input handled correctly")
        else:
            print("  âš ï¸ Empty input not handled")
            
        # Test very long input  
        long_text = "This is a very long news text. " * 100
        result = analyze_news_input(long_text)
        if result.get('category') != 'Error':
            print("  âœ… Long input processed successfully")
        else:
            print("  âš ï¸ Long input caused error")
            
        # Test special characters
        special_text = "Fire at @#$%^&*() location with Ã©mojis ğŸ”¥"
        result = analyze_news_input(special_text)
        if result.get('category') != 'Error':
            print("  âœ… Special characters handled")
        else:
            print("  âš ï¸ Special characters caused error")
            
        return True
        
    except Exception as e:
        print(f"  âŒ Error handling test failed: {e}")
        return False


def run_performance_benchmark():
    """Run basic performance benchmark."""
    print("\nâš¡ Running Performance Benchmark...")
    
    try:
        from backend.analyze_input import analyze_news_input
        
        # Test multiple predictions for timing
        test_text = "Fire broke out in commercial building causing traffic jam"
        iterations = 10
        
        start_time = time.time()
        
        for _ in range(iterations):
            result = analyze_news_input(test_text)
            if 'error' in result:
                print(f"  âŒ Prediction failed during benchmark")
                return False
                
        end_time = time.time()
        avg_time = (end_time - start_time) / iterations
        
        print(f"  â±ï¸ Average prediction time: {avg_time*1000:.2f}ms")
        print(f"  ğŸš€ Predictions per second: {1/avg_time:.1f}")
        
        if avg_time < 1.0:  # Less than 1 second
            print("  âœ… Performance acceptable for real-time usage")
        else:
            print("  âš ï¸ Performance may be slow for real-time usage")
            
        return True
        
    except Exception as e:
        print(f"  âŒ Performance benchmark failed: {e}")
        return False


def main():
    """Run complete integration test suite."""
    print("ğŸ§ª Infosphere ML Pipeline Integration Tests")
    print("=" * 50)
    print(f"ğŸ“… Test run started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Run all tests
    tests = [
        ("Database Connectivity", test_database_connectivity),
        ("Model Training", test_model_training), 
        ("News Classification", test_news_classification),
        ("Backend Integration", test_backend_integration),
        ("Error Handling", test_error_handling),
        ("Performance Benchmark", run_performance_benchmark)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"  âŒ {test_name} test crashed: {e}")
            results[test_name] = False
    
    # Summary
    print("\n" + "=" * 50)
    print("ğŸ“Š TEST SUMMARY")
    print("=" * 50)
    
    passed_tests = sum(results.values())
    total_tests = len(results)
    
    for test_name, passed in results.items():
        status_icon = "âœ…" if passed else "âŒ"
        print(f"{status_icon} {test_name}")
    
    print(f"\nğŸ¯ Overall Result: {passed_tests}/{total_tests} tests passed")
    
    if passed_tests == total_tests:
        print("ğŸ‰ All tests passed! ML pipeline is ready for production.")
        return 0
    elif passed_tests >= total_tests * 0.8:
        print("âš ï¸ Most tests passed. Some issues need attention.")
        return 1
    else:
        print("âŒ Multiple test failures. System needs debugging.")
        return 2


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)